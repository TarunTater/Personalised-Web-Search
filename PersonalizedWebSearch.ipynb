{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Re-ranking of URLs\n",
    "### Solution to Kaggle's Personalized Web Search Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Madhumathi, Tarun Tater, Sushravys G M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge description -- https://www.kaggle.com/c/yandex-personalized-web-search-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files may be found here -- https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about solution, please refer massive_project_report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import re\n",
    "import graphlab.aggregate as agg\n",
    "import time\n",
    "import operator\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the raw data into tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fopen = open(\"train.csv\", \"r\")\n",
    "trans = fopen.readlines()\n",
    "fopen.close()\n",
    "outfile = open(\"trainRevised.csv\", \"w\")\n",
    "for i in range(1000000):\n",
    "    \n",
    "    trans[i] = re.sub(\"\"\"\\n\"\"\", \"\", trans[i],re.I|re.S)\n",
    "    trans[i] = re.sub(\"\"\",\"\"\", \"#\", trans[i],re.I|re.S)\n",
    "    if('M' in trans[i]):\n",
    "        trans[i] = trans[i] + \"NA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\n\"\n",
    "        outfile.write(trans[i])\n",
    "    elif('C' in trans[i]):\n",
    "        trans[i] = trans[i] + \"\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\tNA\\n\"\n",
    "        outfile.write(trans[i])\n",
    "    else:\n",
    "        trans[i] = trans[i] + \"\\n\"\n",
    "        outfile.write(trans[i])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = gl.SFrame.read_csv(\"train.csv\", delimiter = '\\t', header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and working on data of 1000 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listofIndices = {}\n",
    "lId = []\n",
    "prevId = \"0\"\n",
    "e1 = 0\n",
    "l = [e1]\n",
    "lStart = []\n",
    "listOfTime = []\n",
    "\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/1000Users.csv\") as f:\n",
    "    for i in xrange(1):\n",
    "        f.next()\n",
    "    for e, line in enumerate( f ):\n",
    "        if(('M' in line) | ('Q' in line)):\n",
    "            listOfTime.append(-1)\n",
    "        else:\n",
    "            listOfTime.append(int(line.split(\"\\t\")[1]))\n",
    "        if(prevId == line.split(\"\\t\")[0]):\n",
    "            pass\n",
    "        else:\n",
    "            l.append(e+1)\n",
    "            listofIndices[int(prevId)] = l\n",
    "            lId.append(int(prevId))\n",
    "            prevId = line.split(\"\\t\")[0]\n",
    "            e1 = e+1\n",
    "            l = [e1]\n",
    "            lStart.append(int(e1))\n",
    "            \n",
    "l = [e1, (e+2)]\n",
    "listofIndices[int(line.split(\"\\t\")[0])] = l\n",
    "lId.append(int(line.split(\"\\t\")[0]))\n",
    "lastSession = int(line.split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating relevance scores of urls according to user satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevanceList = []#-1 for query and session.\n",
    "relevanceList.append('NA')\n",
    "for i in range(len(listOfTime)-1):\n",
    "    if((listOfTime[i] != -1) & (listOfTime[i+1] == -1)):\n",
    "        relevanceList.append(str(3))\n",
    "    elif((listOfTime[i] > -1) & (listOfTime[i+1] > -1)):\n",
    "        if((listOfTime[i+1] - listOfTime[i]) < 50):\n",
    "            relevanceList.append(str(1))\n",
    "        elif((listOfTime[i+1] - listOfTime[i]) > 399):\n",
    "            relevanceList.append(str(3))\n",
    "        else:\n",
    "            relevanceList.append(str(2))\n",
    "    elif(listOfTime[i] == -1):\n",
    "        relevanceList.append('NA')\n",
    "\n",
    "if(listOfTime[i+1] != -1):\n",
    "    relevanceList.append(str(2))\n",
    "else:\n",
    "    relevanceList.append('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data into readable table format with every Query followed by its 10 result URLs and their relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whichSession = 0 \n",
    "checkNewSession = listofIndices[whichSession][1]#keeps track of when the new session starts\n",
    "pages = []#the urls for a query\n",
    "writeLines = [] #the lines which need to be written when a session completes\n",
    "writeLine = \"\"#a string for each line\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/train1000Users.csv\", \"w\")\n",
    "header = 'userId' + ',' + 'day' + ',' + 'sessionId' + ',' + 'typeOfRecord' + ',' + 'queryId' + ','+ 'urlId' + ',' + 'relevance' + ',' + 'terms' + ',' + 'url1' + ',' + 'url2' + ',' + 'url3' + ',' + 'url4' + ',' + 'url5' + ',' + 'url6' + ',' + 'url7' + ',' + 'url8' + ',' + 'url9' + ',' + 'url10' + '\\n'\n",
    "outfile.write(header)#wrote the header\n",
    "outfile.close()\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/train1000Users.csv\", \"a\")\n",
    "relevance = '0'\n",
    "isPageClicked = False\n",
    "shouldWrite = False\n",
    "\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/1000Users.csv\") as f: \n",
    "    \n",
    "    for e, line in enumerate( f ):\n",
    "        if(e < checkNewSession):#for each session\n",
    "            line = line.split('\\t')\n",
    "            if(line[1] == 'M' ):\n",
    "                userId = line[3]\n",
    "                day = line[2]\n",
    "                sessionId = line[0]\n",
    "                writeLine = \"\"\n",
    "                writeLines = []\n",
    "                newSession = True\n",
    "                isPageClicked = False\n",
    "                timeSession = 0\n",
    "                \n",
    "            else:\n",
    "                if(line[2] == 'Q'):\n",
    "                    if((newSession == False) & (isPageClicked == True)):\n",
    "                        for eachPageNotClicked in pages:\n",
    "                            urlId = eachPageNotClicked\n",
    "                            writeLine = userId + ',' + day + ',' + sessionId + ',' + typeOfRecord + ',' + queryId + ',' + urlId +',' + relevance + ',NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\\n'\n",
    "                            writeLines.append(writeLine)\n",
    "                    \n",
    "                    isPageClicked = False\n",
    "                    \n",
    "                    newSession = False\n",
    "                    newQuery = True\n",
    "                    typeOfRecord = 'Q'\n",
    "                    queryId = line[4]\n",
    "                    urlId = 'NA'\n",
    "                    terms = line[5]\n",
    "                    terms = re.sub(\"\"\",\"\"\", \"#\", terms,re.I|re.S)\n",
    "                    url1 = line[6].split(',')[0]\n",
    "                    url2 = line[7].split(',')[0]\n",
    "                    url3 = line[8].split(',')[0]\n",
    "                    url4 = line[9].split(',')[0]\n",
    "                    url5 = line[10].split(',')[0]\n",
    "                    url6 = line[11].split(',')[0]\n",
    "                    url7 = line[12].split(',')[0]\n",
    "                    url8 = line[13].split(',')[0]\n",
    "                    url9 = line[14].split(',')[0]\n",
    "                    url10 = line[15].split(',')[0]\n",
    "                    pages = [url1, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "                    writeLine = userId + ',' + day + ',' + sessionId + ',' + typeOfRecord + ',' + queryId + ',NA,' + 'NA' + ',' + terms + ',' + url1 + ',' + url2 + ',' + url3 + ',' + url4 + ',' + url5 + ',' + url6 + ',' + url7 + ',' + url8 + ',' + url9 + ',' + url10 + '\\n'\n",
    "                    \n",
    "                else:\n",
    "                    shouldWrite = True#atleast one page was clicked for at least one query in this session\n",
    "                    if(newQuery == True):\n",
    "                        isPageClicked = True\n",
    "                        writeLines.append(writeLine)\n",
    "                    typeOfRecord = 'C'\n",
    "                    urlId = line[4]\n",
    "                    urlId = re.sub(\"\"\"\\n\"\"\",\"\", urlId, re.I|re.S)\n",
    "                    try:\n",
    "                        pages.remove(urlId)\n",
    "                        writeLine = userId + ',' + day + ',' + sessionId + ',' + typeOfRecord + ',' + queryId + ',' + urlId +',' + relevanceList[e] + ',NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\\n'\n",
    "                        newPageClick = True\n",
    "                    except:\n",
    "                        newPageClick = False\n",
    "                        for j in range(len(writeLines)-1,-1,-1):\n",
    "                            if(urlId in writeLines[j]):\n",
    "                                if(writeLines[j].split(',')[6] < relevanceList[e]):\n",
    "                                    writeLines[j] = userId + ',' + day + ',' + sessionId + ',' + typeOfRecord + ',' + queryId + ',' + urlId +',' + relevanceList[e] + ',NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\\n'\n",
    "                                    break\n",
    "                    newQuery = False\n",
    "                    if(newPageClick == True):\n",
    "                        writeLines.append(writeLine)\n",
    "\n",
    "            if( e == (checkNewSession - 1)) :\n",
    "                if(shouldWrite == True):\n",
    "                    if(isPageClicked == True):\n",
    "                        for eachPageNotClicked in pages:\n",
    "                            urlId = eachPageNotClicked\n",
    "                            writeLine = userId + ',' + day + ',' + sessionId + ',' + typeOfRecord + ',' + queryId + ',' + urlId +',' + relevance + ',NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\\n'\n",
    "                            writeLines.append(writeLine)\n",
    "\n",
    "                    for eachLine in writeLines:\n",
    "                        outfile.write(eachLine)    \n",
    "                if(whichSession != lastSession):\n",
    "                    whichSession = whichSession + 1\n",
    "                    checkNewSession = listofIndices[whichSession][1]#update session, \n",
    "                shouldWrite = False\n",
    "\n",
    "outfile.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting 24 days of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userIds = []\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/train24days.csv\", \"w\")\n",
    "header = 'userId' + ',' + 'day' + ',' + 'sessionId' + ',' + 'typeOfRecord' + ',' + 'queryId' + ','+ 'urlId' + ',' + 'relevance' + ',' + 'terms' + ',' + 'url1' + ',' + 'url2' + ',' + 'url3' + ',' + 'url4' + ',' + 'url5' + ',' + 'url6' + ',' + 'url7' + ',' + 'url8' + ',' + 'url9' + ',' + 'url10' + '\\n'\n",
    "outfile.write(header)#wrote the header\n",
    "outfile.close()\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/train24days.csv\", \"a\")\n",
    "\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/train1000Users.csv\") as f:\n",
    "    \n",
    "    for i in xrange(1):\n",
    "        f.next()\n",
    "    \n",
    "    for e, line in enumerate( f ):\n",
    "        if(int(line.split(',')[1]) < 24):\n",
    "            outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting 3 days of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userIds = []\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/test3days.csv\", \"w\")\n",
    "header = 'userId' + ',' + 'day' + ',' + 'sessionId' + ',' + 'typeOfRecord' + ',' + 'queryId' + ','+ 'urlId' + ',' + 'relevance' + ',' + 'terms' + ',' + 'url1' + ',' + 'url2' + ',' + 'url3' + ',' + 'url4' + ',' + 'url5' + ',' + 'url6' + ',' + 'url7' + ',' + 'url8' + ',' + 'url9' + ',' + 'url10' + '\\n'\n",
    "outfile.write(header)#wrote the header\n",
    "outfile.close()\n",
    "outfile = open(\"/home/madhumathi/MassiveProject/MassiveData/test3days.csv\", \"a\")\n",
    "\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/train1000Users.csv\") as f:\n",
    "    \n",
    "    for i in xrange(1):\n",
    "        f.next()\n",
    "    \n",
    "    for e, line in enumerate( f ):\n",
    "        if(int(line.split(',')[1]) >= 24):\n",
    "            if(int(line.split(',')[0]) in users):\n",
    "                outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the train and test data into SFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train24days = gl.SFrame.read_csv(\"/home/madhumathi/MassiveProject/MassiveData/train24days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test3days = gl.SFrame.read_csv(\"/home/madhumathi/MassiveProject/MassiveData/test3days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a list of all users, queries and URLs from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = list(train24days['userId'].unique())\n",
    "queries = list(train24days['queryId'].unique())\n",
    "urls = list(train24days['urlId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the User-URL matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userUrlMatrix = np.array([[0.0 for i in range(len(urls))] for i in range(len(users))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = '0'\n",
    "urlRelTemp = {}\n",
    "urlCountTemp = {}\n",
    "for i in urls:\n",
    "    urlCountTemp[i] = 0\n",
    "    urlRelTemp[i] = 0\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/train24days.csv\") as f: \n",
    "    for i in xrange(1):\n",
    "        f.next()\n",
    "    for e, line in enumerate( f ):\n",
    "        lineRead = line.split(',')\n",
    "        if(lineRead[0]!=user):\n",
    "            for  i in urls:\n",
    "                if (urlCountTemp[i] > 0):\n",
    "                    #print user, i, urlRelTemp[i], urlCountTemp[i]\n",
    "                    score = (urlRelTemp[i]*1.0)/urlCountTemp[i]\n",
    "                else:\n",
    "                    score = 0\n",
    "                userUrlMatrix[users.index(int(user))][urls.index(i)] = score\n",
    "            user = lineRead[0]\n",
    "            urlRelTemp = {}\n",
    "            urlCountTemp = {}\n",
    "            for i in urls:\n",
    "                urlCountTemp[i] = 0\n",
    "                urlRelTemp[i] = 0\n",
    "        else:\n",
    "            if(lineRead[3] == 'C'):\n",
    "                urlRelTemp[int(lineRead[5])] += int(lineRead[6])\n",
    "                urlCountTemp[int(lineRead[5])] += 1\n",
    "for  i in urls:\n",
    "    if (urlCountTemp[i] > 0):\n",
    "        #print user, i, urlRelTemp[i], urlCountTemp[i]\n",
    "        score = (urlRelTemp[i]*1.0)/urlCountTemp[i]\n",
    "    else:\n",
    "        score = 0\n",
    "    userUrlMatrix[users.index(int(user))][urls.index(i)] = score      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorizing the User-URL matrix into User-Topic and URL-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NMF(n_components=500, init='nndsvd', random_state=0)\n",
    "model.fit(userUrlMatrix)\n",
    "urlTopic = model.components_\n",
    "urlTopic = np.asmatrix(urlTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    binv = np.linalg.pinv(urlTopic)\n",
    "except np.linalg.LinAlgError:\n",
    "    # Not invertible. Skip this one.\n",
    "    print \"non invertible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userTopic = np.array(userUrlMatrix*binv)\n",
    "userTopic[userTopic < 0.000000000001] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Query-URL matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryUrlMatrix = np.array([[0.0 for i in range(len(urls))] for j in range(len(queries))])\n",
    "trainc = train24days[train24days['typeOfRecord'] == 'C']\n",
    "train = trainc.groupby(key_columns = ['queryId','urlId'],operations = {'count' : agg.COUNT})\n",
    "trainPandas = gl.SFrame.to_dataframe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in trainPandas.iterrows():\n",
    "    uindx = urls.index(row['urlId'])\n",
    "    qindx = queries.index(row['queryId'])\n",
    "    queryUrlMatrix[qindx][uindx] = row['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorizing the Query-URL matrix into Query-Topic and URL-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NMF(n_components=500, init='nndsvd', random_state=0)\n",
    "model.fit(queryUrlMatrix)\n",
    "urlTopic = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSPR matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tspr = userTopic*urlTopic\n",
    "tspr = np.array(tspr)\n",
    "# Normalizing the tspr matrix\n",
    "tspr = (tspr - tspr.mean())/ tspr.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving ranks to URL positions returned by search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positionRanks = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "positionRanks = (positionRanks - positionRanks.mean())/ positionRanks.var()\n",
    "positionRanks = positionRanks+(-1*positionRanks.min()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Getting Navigational Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOfqids = {}\n",
    "check = 0\n",
    "numberOfuser = 0\n",
    "with open(\"/home/madhumathi/MassiveProject/MassiveData/train.csv\") as f:\n",
    "    for e, line in enumerate( f ):\n",
    "        if(line.split('\\t')[1] == 'M'):\n",
    "            if(int(line.split('\\t')[2]) < 7):\n",
    "                numberOfuser = numberOfuser + 1\n",
    "                check = 1\n",
    "                writeLine = line.split('\\t')[0] + ',' + line.split('\\t')[1] + ',' + line.split('\\t')[2] + ',' + line.split('\\t')[3] + '\\n'\n",
    "            else:\n",
    "                check = 0\n",
    "        elif(check == 1):\n",
    "            if(line.split('\\t')[2] == 'Q'):\n",
    "                try:\n",
    "                    listOfqids[int(line.split('\\t')[4])] = listOfqids[int(line.split('\\t')[4])] + 1\n",
    "                except:\n",
    "                    listOfqids[int(line.split('\\t')[4])] = 1\n",
    "                terms = line.split('\\t')[5]\n",
    "                terms = re.sub(\"\"\",\"\"\", \"#\", terms,re.I|re.S)\n",
    "                \n",
    "                writeLine = line.split('\\t')[0] + ',' + line.split('\\t')[1] + ',' + line.split('\\t')[2] + ',' + line.split('\\t')[3] + ',' + line.split('\\t')[4] + ',' + terms + ',' + (line.split('\\t')[6]).split(',')[0] + ',' + (line.split('\\t')[7]).split(',')[0] + ',' + (line.split('\\t')[8]).split(',')[0] + ',' + (line.split('\\t')[9]).split(',')[0] + ',' + (line.split('\\t')[10]).split(',')[0] + ',' + (line.split('\\t')[11]).split(',')[0] + ',' + (line.split('\\t')[12]).split(',')[0] + ',' + (line.split('\\t')[13]).split(',')[0] + ',' + (line.split('\\t')[14]).split(',')[0] + ',' + (line.split('\\t')[15]).split(',')[0] + '\\n'\n",
    "            elif(line.split('\\t')[2] == 'C'):\n",
    "                writeLine = line.split('\\t')[0] + ',' + line.split('\\t')[1] + ',' + line.split('\\t')[2] + ',' + line.split('\\t')[3] + ',' + line.split('\\t')[4]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the 3 days data using NDCG measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = gl.SFrame.to_dataframe(test3days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndcg = []\n",
    "count = 0\n",
    "for index, row in test.iterrows():\n",
    "    if (row['typeOfRecord'] == 'Q'):   \n",
    "        count = 0\n",
    "        ranks = []\n",
    "        relevances = []\n",
    "        relevanceHash = {}\n",
    "        user = int(row['userId'])\n",
    "        resultList = [row['url1'], row['url2'], row['url3'], row['url4'], row['url5'], row['url6'], row['url7'], row['url8'], row['url9'], row['url10']]\n",
    "        resultList = [int(j) for j in resultList]\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        if (int(row['queryId']) not in navigationalQueries):\n",
    "            for i in range(0,len(resultList)):\n",
    "                u = resultList[i]\n",
    "                if(user not in users):\n",
    "                    t1 = 0\n",
    "                elif(u not in urls):\n",
    "                    t1 = 0\n",
    "                else :\n",
    "                    t1 = tspr[users.index(user)][urls.index(u)]\n",
    "                t2 = positionRanks[i]\n",
    "                rank = (t1*1.0)/t2\n",
    "                ranks.append((u, rank))\n",
    "            sorted_urls = sorted(ranks, key=lambda x: x[1], reverse=True)\n",
    "            personalisedResult = [k[0] for k in sorted_urls]\n",
    "            \n",
    "        else:\n",
    "            personalisedResult = resultList\n",
    "    else:\n",
    "        count = count+1\n",
    "        relevanceHash[int(row['urlId'])] = int(row['relevance'])\n",
    "        relevances.append((int(row['urlId']), int(row['relevance'])))\n",
    "        if(count == 10):\n",
    "            sorted_relevance_urls = sorted(relevances, key=lambda x: x[1], reverse=True)\n",
    "            relevanceResult = [k[0] for k in sorted_relevance_urls]\n",
    "            for i in range(0,10):\n",
    "                idcg = idcg+((pow(2, sorted_relevance_urls[i][1]) - 1)*1.0/math.log(i+2,2))\n",
    "                dcg = dcg+((pow(2, relevanceHash[personalisedResult[i]]) - 1)*1.0/math.log(i+2,2))\n",
    "            ndcg.append((dcg*1.0)/idcg)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
